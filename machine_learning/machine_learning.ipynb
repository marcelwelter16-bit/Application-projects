{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5b2b4ea",
   "metadata": {},
   "source": [
    "# Machine learning with artificial neural networks\n",
    "\n",
    "In this project, an application of machine learning models, using a simulated dataset, will be implemented.\\\n",
    "The dataset was taken from [openml](https://www.openml.org/search?type=data&sort=runs&id=4532&status=active). It is a tabular dataset consisting of 28 features that are (functions of) kinematic properties.\n",
    "Besides, it contains a binary target, denoting if a process is tied to a higgs boson (signal) or another process (background).\n",
    "For the sake of a faster runtime and independence of external sources, a sample dataset was created. \\\n",
    "An artificial neural network will be used in order to determine if a process is signal or background based on the given features.\n",
    "\n",
    "The entire notebook should run in about 1 min. (with the sample dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef75162",
   "metadata": {},
   "source": [
    "## Part 1: Obtaining the data\n",
    "- Download of the [Higgs dataset](https://www.openml.org/search?type=data&sort=runs&id=4532&status=active) or load a smaller sample dataset.\n",
    "- The data is saved as a a pandas DataFrame.\n",
    "\n",
    "## Part2: Cleaning the data\n",
    "- Removal of rows with missing values, infinite values and duplicate rows\n",
    "\n",
    "## Part 3: Data Summary and Statistics\n",
    "- Presentation of some basic information of the data.\n",
    "- Plot of the distributions of `lepton_pT`, `lepton_eta`, and `lepton_phi` as histograms for the signal and background events.\n",
    "\n",
    "## Part 4: Pre-Process Features\n",
    "- Re-seperation of features and target and preprocessing of the features.\n",
    "\n",
    "## Part 5: Split Dataset\n",
    "- 70% will be used for training, 15% for testing and 15% for validation.\n",
    "\n",
    "## Part 6: Simple MLP Model with Keras\n",
    "- Definition and compiling of a simple simple Multi-Layer Perceptron (MLP) model with [Keras](https://keras.io/).\n",
    "- Training of the model with the given data.\n",
    "- Plot of the training and validation loss over the training epochs as well as the ROC-curve.\n",
    "- Evaluation of the model on the test set.\n",
    "\n",
    "## Part 7: Feature Importance\n",
    "- Training of the MLP while dropping features.\n",
    "- Ranking of the first 10 features based on importance. (Not all features to keep the runtime short)\n",
    "\n",
    "## Part 8: Brute Force Single Feature Attack on MLP\n",
    "- For 5 correctly classified samples each feature will be varied over its valid range (min/max values from the training set).\n",
    "- Misclassifications after changing a single feature will be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c997d-292a-48ad-b228-b2ce1b33f6a4",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175f9ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml  # to download the full dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# to prepare data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# for the ROC-curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# to construct the neural network\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 14})  # larger text in plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9450102",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = False\n",
    "\n",
    "\n",
    "if full_dataset == True:\n",
    "    # download the full Higgs dataset from openml (98 050 events)\n",
    "    higgs_dataset = openml.datasets.get_dataset(4532)\n",
    "\n",
    "    # seperate the data into features/target and combine into a pandas DataFrame\n",
    "    X_higgs, y_higgs, _, _ = higgs_dataset.get_data(target=higgs_dataset.default_target_attribute)\n",
    "    df = pd.DataFrame(X_higgs)  # feature data\n",
    "    df['target'] = y_higgs  # add target-data\n",
    "\n",
    "else:\n",
    "    # load the provided sample dataset (only 10 000 events of the full dataset)\n",
    "    df = pd.read_csv(\"data/higgs_sample.csv\")\n",
    "\n",
    "\n",
    "# extract column names for parts 7 and 8\n",
    "features = df.columns.to_numpy()\n",
    "\n",
    "# print some basic information\n",
    "print(f'Shape: \\n{df.shape}\\n')\n",
    "print(f'Head: \\n{df.head()}\\n')\n",
    "print(f'Columns: \\n{features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428a9677-281a-472f-b357-2ba5576e476d",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b63815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-cleaning shape\n",
    "print(f'Original shape: \\n{df.shape}\\n')\n",
    "\n",
    "# remove NaN-entries and infinities\n",
    "df = df[np.isfinite(df).all(axis=1)]\n",
    "\n",
    "# remove identical rows\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# post-cleaning shape\n",
    "print(f'Cleaned shape: \\n{df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f583c56b",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d26a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print relevant information\n",
    "print(f'Names and data types: \\n{df.dtypes}\\n')\n",
    "print(f'Basic statistics: \\n{df.describe()}\\n')\n",
    "print(df['target'].value_counts())\n",
    "\n",
    "\n",
    "# plot histograms\n",
    "\n",
    "nbins = 100  # bin-number\n",
    "\n",
    "fig, ax = plt.subplots(3, 1, figsize=(10,8))\n",
    "fig.tight_layout(pad=4.0)\n",
    "\n",
    "# data\n",
    "ax[0].hist(df['lepton_pT'][df['target'] == 1], bins=nbins, label='signal', alpha=.7)\n",
    "ax[0].hist(df['lepton_pT'][df['target'] == 0], bins=nbins, label='background', alpha=.7)\n",
    "\n",
    "ax[1].hist(df['lepton_eta'][df['target'] == 1], bins=nbins, label='signal', alpha=.7)\n",
    "ax[1].hist(df['lepton_eta'][df['target'] == 0], bins=nbins, label='background', alpha=.7)\n",
    "\n",
    "ax[2].hist(df['lepton_phi'][df['target'] == 1], bins=nbins, label='signal', alpha=.7)\n",
    "ax[2].hist(df['lepton_phi'][df['target'] == 0], bins=nbins, label='background', alpha=.7)\n",
    "\n",
    "# appearance\n",
    "ax[0].set_title(r'$\\mathtt{pT}$-distribution')\n",
    "ax[0].set_xlabel(r'$\\mathtt{lepton\\_pT}$')\n",
    "ax[0].set_ylabel('Counts')\n",
    "ax[0].grid(True, 'both')\n",
    "ax[0].legend(loc='upper right')\n",
    "        \n",
    "ax[1].set_title(r'$\\mathtt{eta}$-distribution')\n",
    "ax[1].set_xlabel(r'$\\mathtt{lepton\\_eta}$')\n",
    "ax[1].set_ylabel('Counts')\n",
    "ax[1].grid(True, 'both')\n",
    "ax[1].legend(loc='upper right')\n",
    "\n",
    "ax[2].set_title(r'$\\mathtt{phi}$-distribution')\n",
    "ax[2].set_xlabel(r'$\\mathtt{lepton\\_phi}$')\n",
    "ax[2].set_ylabel('Counts')\n",
    "ax[2].grid(True, 'both')\n",
    "ax[2].legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53186db9",
   "metadata": {},
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc565870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reseparate DataFrame and ensure integer classification\n",
    "X_higgs = df.iloc[:, :-1]\n",
    "y_higgs = df.iloc[:, -1].astype(int)\n",
    "\n",
    "# mean and std before preprocessing (using \".values\" to print only numerical data from pandas series)\n",
    "print(f'Averages before preprocessing: \\n{np.mean(X_higgs, axis=0).values}\\n')\n",
    "print(f'Standard deviations before preprocessing: \\n{np.std(X_higgs, axis=0).values}\\n')\n",
    "\n",
    "# preprocessing\n",
    "X_higgs = StandardScaler().fit_transform(X_higgs)\n",
    "\n",
    "# mean and std after preprocessing\n",
    "print(f'Averages after preprocessing: \\n{np.mean(X_higgs, axis=0)}\\n')\n",
    "print(f'Standard deviations after preprocessing: \\n{np.std(X_higgs, axis=0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d96edb-189c-4c66-bc91-c13e06af7d3a",
   "metadata": {},
   "source": [
    "The averages before preprocessing fluctuate over almost four orders of magnitude which is reduced to about one order by preprocessing. Meanwhile the standard deviations fluctuate over about one order of magnitude at the beginning and are all equal in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1b4cf1",
   "metadata": {},
   "source": [
    "## Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddb2f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data using train_test_split (must be done twice since data is only split into 2 parts)\n",
    "# split into training and non-training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_higgs, y_higgs,\n",
    "                                                    test_size=0.3, random_state=42)\n",
    "# split into test and validation\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test,\n",
    "                                                test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training features' shape: {X_train.shape}\")\n",
    "print(f\"Training targets' shape: {y_train.shape}\")\n",
    "\n",
    "print(f\"Test features' shape: {X_test.shape}\")\n",
    "print(f\"Test targets' shape: {y_test.shape}\")\n",
    "\n",
    "print(f\"Validation features' shape: {X_val.shape}\")\n",
    "print(f\"Validation targets' shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9a7324",
   "metadata": {},
   "source": [
    "## Part 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f61c8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim=28, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model summary\n",
    "print(model.summary)\n",
    "\n",
    "# training the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=25,\n",
    "    batch_size=512\n",
    ")\n",
    "\n",
    "# plot losses over epochs\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Neural Network Training vs. Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# evaluate with test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'\\n Test accuracy: {test_acc}')\n",
    "\n",
    "\n",
    "# consider ROC curve\n",
    "\n",
    "# Predict probabilities (as flat array)\n",
    "y_pred_nn = model.predict(X_val).ravel()\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr_nn, tpr_nn, _ = roc_curve(y_val, y_pred_nn)\n",
    "roc_auc_nn = auc(fpr_nn, tpr_nn)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot(fpr_nn, tpr_nn, color='blue', label=f'Neural Network (AUC = {roc_auc_nn:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve (Neural Network Classifier)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b808d27",
   "metadata": {},
   "source": [
    "## Part 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb32a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simpler model to reduce runtime\n",
    "model_light = Sequential([\n",
    "    Dense(32, input_dim=27, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model_light.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# create an array to be filled with accuracies\n",
    "dropped_acc = np.zeros(10)\n",
    "\n",
    "# loop through features\n",
    "for i in range(10): \n",
    "    # training the model, where np.delete() is used to drop certain features\n",
    "    history = model_light.fit(\n",
    "        np.delete(X_train, i, axis=1), y_train,\n",
    "        validation_data=(np.delete(X_val, i, axis=1), y_val),\n",
    "        epochs=15, \n",
    "        batch_size=512,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # extract accuracies\n",
    "    _, dropped_acc[i] = model_light.evaluate(np.delete(X_test, i, axis=1), y_test)\n",
    "    \n",
    "# sort features corresponding to increasing accuracies, i.e. decreasing importance\n",
    "print('\\n The first 10 features and corresponding accuracies in decreasing importance:')\n",
    "print(pd.Series(dropped_acc, features[9::-1]).sort_values())  # print as pd.Series for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed98f71",
   "metadata": {},
   "source": [
    "## Part 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d51aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate probability for a sample to be signal based on the model\n",
    "prob = model.predict(X_test, verbose=0).flatten()\n",
    "\n",
    "# get indices where the difference between probability and target is < 0.5\n",
    "args_correct = np.where(np.abs(prob - y_test) < .5 )[0]\n",
    "\n",
    "# select 5 correctly classified samples\n",
    "args_selected = np.random.choice(args_correct, 5)\n",
    "\n",
    "# extract and print valid ranges\n",
    "features_min = np.min(X_train, axis=0)\n",
    "features_max = np.max(X_train, axis=0)\n",
    "\n",
    "print('Features and the corresponding valid ranges:')\n",
    "print(pd.DataFrame({'feature': features[:-1],\n",
    "                    'min': features_min,\n",
    "                    'max': features_max}).to_string(index=False))\n",
    "\n",
    "# create an array of possible feature values\n",
    "steps = 10\n",
    "possible_feature_vals = np.linspace(features_min, features_max, steps)  # shape: (steps, 28)\n",
    "\n",
    "# define dictionary to be filled later\n",
    "misclass_details = {'Sample no.': [], 'Feature': [],\n",
    "                    'Old value': [], 'New value': [],\n",
    "                    'Prediction': [], 'True label': []}\n",
    "\n",
    "# loop over samples\n",
    "for arg in args_selected:\n",
    "    sample = X_test[arg]\n",
    "    stop = False  # will be used to stop changing features after misclassification\n",
    "    \n",
    "    # change each feature of each sample\n",
    "    for feature_ind in range(28):\n",
    "        for step_number in range(steps):\n",
    "            \n",
    "            # copy the sample to implement changes\n",
    "            test_sample = sample.copy()\n",
    "            test_sample[feature_ind] = possible_feature_vals[step_number, feature_ind]\n",
    "            \n",
    "            new_prob = model.predict(test_sample[np.newaxis, :], verbose=0)  # mind dimensions\n",
    "            \n",
    "            # check for misclassification\n",
    "            if np.abs(new_prob - y_test.iloc[arg]) > .5: \n",
    "                misclass_details['Sample no.'].append(arg)\n",
    "                misclass_details['Feature'].append(features[feature_ind])\n",
    "                misclass_details['Old value'].append(sample[feature_ind])\n",
    "                misclass_details['New value'].append(test_sample[feature_ind])\n",
    "                misclass_details['Prediction'].append(new_prob[0][0])\n",
    "                misclass_details['True label'].append(y_test.iloc[arg])\n",
    "                \n",
    "                stop = True\n",
    "                break   # break step_number loop\n",
    "                \n",
    "        if stop:\n",
    "            break   # break feature_ind loop\n",
    "            \n",
    "# print results\n",
    "print(f\"\\n \\n{len(misclass_details['Sample no.'])} out of 5 samples were misclassified:\\n\")\n",
    "print(pd.DataFrame(misclass_details).to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
